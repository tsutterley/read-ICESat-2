{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Plot-ATL11-Tidal-Histograms\n",
    "\n",
    "Calculates histograms of ICESat-2 land ice elevation differences corrected for different tide models over Antarctic ice shelves\n",
    "\n",
    "#### Python Dependencies\n",
    "- [numpy: Scientific Computing Tools For Python](https://numpy.org)\n",
    "- [scipy: Scientific Tools for Python](https://docs.scipy.org/doc/)\n",
    "- [h5py: Python interface for Hierarchal Data Format 5 (HDF5)](https://h5py.org)\n",
    "- [pyproj: Python interface to PROJ library](https://pypi.org/project/pyproj/)\n",
    "- [matplotlib: Python 2D plotting library](http://matplotlib.org/)\n",
    "\n",
    "#### Program Dependencies\n",
    "- read_ICESat2_ATL11.py: reads ICESat-2 annual land ice height data files\n",
    "- time.py: utilities for calculating time operations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Load necessary modules for running the notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pyproj\n",
    "import datetime\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib\n",
    "matplotlib.rcParams['axes.linewidth'] = 2.0\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Helvetica']\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.offsetbox\n",
    "import ipywidgets as widgets\n",
    "from icesat2_toolkit.read_ICESat2_ATL11 import read_HDF5_ATL11,read_HDF5_ATL11_pair\n",
    "import icesat2_toolkit.time"
   ]
  },
  {
   "source": [
    "#### Get current list of available cycles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycles():\n",
    "    cycle_length = 2\n",
    "    # number of GPS seconds between the GPS epoch and ATLAS SDP epoch\n",
    "    atlas_sdp_gps_epoch = 1198800018.0\n",
    "    # number of GPS seconds since the GPS epoch for first ATLAS data point\n",
    "    atlas_gps_start_time = atlas_sdp_gps_epoch + 24710205.39202261\n",
    "    epoch1 = datetime.datetime(1980,1,6,0,0,0)\n",
    "    epoch2 = datetime.datetime(1970,1,1,0,0,0)\n",
    "    # get the total number of seconds since the start of ATLAS and now\n",
    "    delta_time_epochs = (epoch2 - epoch1).total_seconds()\n",
    "    atlas_UNIX_start_time = (atlas_gps_start_time - delta_time_epochs)\n",
    "    present_time = datetime.datetime.now().timestamp()\n",
    "    # divide total time by cycle length to get the maximum number of orbital cycles\n",
    "    ncycles = np.ceil((present_time - atlas_UNIX_start_time)/(86400*91)).astype('i')\n",
    "    return [str(c+1).zfill(cycle_length) for c in range(ncycles)]"
   ]
  },
  {
   "source": [
    "#### Set working data directory and histogram parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regional plot parameters\n",
    "# x and y limit (in projection)\n",
    "region_xlimit = {}\n",
    "region_ylimit = {}\n",
    "# Antarctica (AIS)\n",
    "region_xlimit['AIS'] = (-3100000,3100000)\n",
    "region_ylimit['AIS'] = (-2600000,2600000)\n",
    "# Ronne/Filchner Ice Shelf\n",
    "region_xlimit['FRIS'] = (-1535000,-520000)\n",
    "region_ylimit['FRIS'] = (77500,1092500)\n",
    "# Ross Ice Shelf\n",
    "region_xlimit['RIS'] = (-740000,520000)\n",
    "region_ylimit['RIS'] = (-1430000,-300000)\n",
    "# Amery Ice Shelf\n",
    "region_xlimit['AMIS'] = (1630000,2310000)\n",
    "region_ylimit['AMIS'] = (530000,880000)\n",
    "# Larsen-C Ice Shelf\n",
    "region_xlimit['LCIS'] = (-2470000,-2050000)\n",
    "region_ylimit['LCIS'] = (895000,1325000)\n",
    "# Larsen-D Ice Shelf\n",
    "region_xlimit['LDIS'] = (-2130000,-1595000)\n",
    "region_ylimit['LDIS'] = (880000,1165000)\n",
    "# George VI Ice Shelf\n",
    "region_xlimit['G6IS'] = (-2230000,-1685000)\n",
    "region_ylimit['G6IS'] = (320000,830000)\n",
    "# Abbot Ice Shelf\n",
    "region_xlimit['ABIS'] = (-2000000,-1800000)\n",
    "region_ylimit['ABIS'] = (-460000,100000)\n",
    "# Pine Island Ice Shelf\n",
    "region_xlimit['PIIS'] = (-1695000,-1510000)\n",
    "region_ylimit['PIIS'] = (-380000,-230000)\n",
    "# Thwaites Glacier Tongue\n",
    "region_xlimit['THWGT'] = (-1630000,-1480000)\n",
    "region_ylimit['THWGT'] = (-525000,-370000)\n",
    "# Dotson/Crosson Ice Shelf\n",
    "region_xlimit['DCIS'] = (-1640000,-1460000)\n",
    "region_ylimit['DCIS'] = (-715000,-525000)\n",
    "# Wilkins Ice Shelf\n",
    "region_xlimit['WLKIS'] = (-2180000,-1900000)\n",
    "region_ylimit['WLKIS'] = (530000,795000)\n",
    "# Wordie (Prospect) Ice Shelf\n",
    "region_xlimit['WRDIS'] = (-2115000,-2042500)\n",
    "region_ylimit['WRDIS'] = (830000,895000)\n",
    "# Venable Ice Shelf\n",
    "region_xlimit['VBLIS'] = (-1895000,-1800000)\n",
    "region_ylimit['VBLIS'] = (22000,151000)\n",
    "\n",
    "# set the directory with ICESat-2 data\n",
    "dirText = widgets.Text(\n",
    "    value=os.getcwd(),\n",
    "    description='Directory',\n",
    "    disabled=False\n",
    ")\n",
    "# set the ICESat-2 ATL11 data release\n",
    "releaseDropdown = widgets.Dropdown(\n",
    "    options=['001','002'],\n",
    "    value='002',\n",
    "    description='Release',\n",
    "    disabled=False\n",
    ")\n",
    "# set the ICESat-2 start and end cycles\n",
    "all_cycles = cycles()\n",
    "cycleSelect = widgets.SelectionRangeSlider(\n",
    "    options=all_cycles,\n",
    "    index=(2,len(all_cycles)-1),\n",
    "    description='Cycles',\n",
    "    disabled=False\n",
    ")\n",
    "# set the ICESat-2 granule regions\n",
    "granuleSelect = widgets.SelectMultiple(\n",
    "    options=np.arange(1,15),\n",
    "    value=[10,11,12],\n",
    "    description='Granules',\n",
    "    disabled=False\n",
    ")\n",
    "# set the region to calculate histograms\n",
    "regions = region_xlimit.keys()\n",
    "regionDropdown = widgets.Dropdown(\n",
    "    options=regions,\n",
    "    value='FRIS',\n",
    "    description='Region',\n",
    "    disabled=False\n",
    ")\n",
    "# set the differencing method for histograms\n",
    "methodDropdown = widgets.Dropdown(\n",
    "    options=['AT','XT'],\n",
    "    value='AT',\n",
    "    description='Method',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# display widgets for setting parameters\n",
    "widgets.VBox([dirText,releaseDropdown,cycleSelect,granuleSelect,\n",
    "    regionDropdown,methodDropdown])"
   ]
  },
  {
   "source": [
    "#### Find indices of common reference points between two lists\n",
    "Determines which along-track points correspond with the across-track"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_reference_points(XT, AT):\n",
    "    ind2 = np.squeeze([np.flatnonzero(AT == p) for p in XT])\n",
    "    return ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# get values from widgets\n",
    "base_dir = os.path.expanduser(dirText.value)\n",
    "RELEASE = releaseDropdown.value\n",
    "CYCLES = cycleSelect.value\n",
    "GRANULES = granuleSelect.value\n",
    "REGION = regionDropdown.value\n",
    "# read crossovers from ATL11 files\n",
    "METHOD = methodDropdown.value\n",
    "CROSSOVERS = (METHOD == 'XT')\n",
    "# tide models to use\n",
    "TIDE_MODELS = ['CATS2008','TPXO9-atlas-v3','GOT4.10','FES2014']\n",
    "# height threshold (filter points below 0m elevation)\n",
    "THRESHOLD = 0.0\n",
    "# time threshold for crossover differences\n",
    "DAYS = 10.0\n",
    "\n",
    "# histogram parameters\n",
    "w = 0.01\n",
    "vmin,vmax=(-8,8)\n",
    "b1 = np.arange(vmin,vmax+w,w)\n",
    "b2 = (b1[1:] + b1[0:-1])/2.0\n",
    "nbins = np.int((vmax-vmin)/w)\n",
    "# total difference histogram for each tide model\n",
    "hist = dict(Uncorrected=np.zeros((nbins)))\n",
    "for m in TIDE_MODELS:\n",
    "    hist[m] = np.zeros((nbins))\n",
    "\n",
    "# find ICESat-2 HDF5 files in the subdirectory for product and release\n",
    "regex_track = '|'.join(['{0:04d}'.format(T) for T in range(1,1388)])\n",
    "regex_granule = '|'.join(['{0:02d}'.format(G) for G in GRANULES])\n",
    "# compile regular expression operator for extracting data from files\n",
    "args = (regex_track,regex_granule,CYCLES[0],CYCLES[1],RELEASE)\n",
    "regex_pattern = (r'(processed_)?(ATL\\d{{2}})_({0})({1})_({2})({3})_'\n",
    "    r'({4})_(\\d{{2}})(.*?).h5$')\n",
    "rx = re.compile(regex_pattern.format(*args), re.VERBOSE)\n",
    "# associated file format\n",
    "file_format = '{0}_{1}_{2}_{3}{4}_{5}{6}_{7}_{8}{9}.h5'\n",
    "# HDF5 group name for across-track data\n",
    "XT = 'crossing_track_data'\n",
    "\n",
    "# projections for converting lat/lon to polar stereographic\n",
    "crs1 = pyproj.CRS.from_string(\"epsg:{0:d}\".format(4326))\n",
    "crs2 = pyproj.CRS.from_string(\"epsg:{0:d}\".format(3031))\n",
    "transformer = pyproj.Transformer.from_crs(crs1, crs2, always_xy=True)\n",
    "\n",
    "# find all input ATL11 files\n",
    "FILE1 = [os.path.join(base_dir,f) for f in os.listdir(base_dir)\n",
    "    if bool(rx.match(f))]\n",
    "# total number of valid segments for differencing method\n",
    "total_valid = 0\n",
    "# for each file in the cycle\n",
    "for f1 in sorted(FILE1):\n",
    "    # extract parameters from file\n",
    "    SUB,PRD,TRK,GRAN,SCYC,ECYC,RL,VERS,AUX = rx.findall(f1).pop()\n",
    "    # read ICESat-2 file\n",
    "    try:\n",
    "        mds1,attrs1,pairs1 = read_HDF5_ATL11(f1, VERBOSE=True,\n",
    "            ATTRIBUTES=True, CROSSOVERS=CROSSOVERS)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # for each beam in the file\n",
    "    for ptx in pairs1:\n",
    "        # extract along-track and across-track variables\n",
    "        ref_pt = {}\n",
    "        latitude = {}\n",
    "        longitude = {}\n",
    "        delta_time = {}\n",
    "        h_corr = {}\n",
    "        quality_summary = {}\n",
    "        tide_ocean = {m:{} for m in TIDE_MODELS}\n",
    "        ib = {}\n",
    "        groups = ['AT']\n",
    "        # shape of along-track data\n",
    "        n_points,n_cycles = mds1[ptx]['delta_time'].shape\n",
    "        # along-track (AT) reference point, latitude, longitude and time\n",
    "        ref_pt['AT'] = mds1[ptx]['ref_pt'].copy()\n",
    "        latitude['AT'] = np.ma.array(mds1[ptx]['latitude'],\n",
    "            fill_value=attrs1[ptx]['latitude']['_FillValue'])\n",
    "        latitude['AT'].mask = (latitude['AT'] == latitude['AT'].fill_value)\n",
    "        longitude['AT'] = np.ma.array(mds1[ptx]['longitude'],\n",
    "            fill_value=attrs1[ptx]['longitude']['_FillValue'])\n",
    "        longitude['AT'].mask = (longitude['AT'] == longitude['AT'].fill_value)\n",
    "        delta_time['AT'] = np.ma.array(mds1[ptx]['delta_time'],\n",
    "            fill_value=attrs1[ptx]['delta_time']['_FillValue'])\n",
    "        delta_time['AT'].mask = (delta_time['AT'] == delta_time['AT'].fill_value)\n",
    "        # corrected height\n",
    "        h_corr['AT'] = np.ma.array(mds1[ptx]['h_corr'],\n",
    "            fill_value=attrs1[ptx]['h_corr']['_FillValue'])\n",
    "        h_corr['AT'].mask = (h_corr['AT'] == h_corr['AT'].fill_value)\n",
    "        # quality summary\n",
    "        quality_summary['AT'] = (mds1[ptx]['quality_summary'] == 0)\n",
    "        # ocean corrections\n",
    "        for m in TIDE_MODELS:\n",
    "            tide_ocean[m]['AT'] = np.ma.zeros((n_points,n_cycles),\n",
    "                fill_value=attrs1[ptx]['cycle_stats']['tide_ocean']['_FillValue'])\n",
    "            tide_ocean[m]['AT'].mask = np.zeros((n_points,n_cycles),dtype=bool)\n",
    "        ib['AT'] = np.ma.array(mds1[ptx]['cycle_stats']['dac'],\n",
    "            fill_value=attrs1[ptx]['cycle_stats']['dac']['_FillValue'])\n",
    "        ib['AT'].mask = (ib['AT'] == ib['AT'].fill_value)\n",
    "        # if running ATL11 crossovers\n",
    "        if CROSSOVERS:\n",
    "            # add to group\n",
    "            groups.append('XT')\n",
    "            # shape of across-track data\n",
    "            n_cross, = mds1[ptx][XT]['delta_time'].shape\n",
    "            # across-track (XT) reference point\n",
    "            ref_pt['XT'] = mds1[ptx][XT]['ref_pt'].copy()\n",
    "            # across-track (XT) latitude, longitude and time\n",
    "            latitude['XT'] = np.ma.array(mds1[ptx][XT]['latitude'],\n",
    "                fill_value=attrs1[ptx][XT]['latitude']['_FillValue'])\n",
    "            latitude['XT'].mask = (latitude['XT'] == latitude['XT'].fill_value)\n",
    "            longitude['XT'] = np.ma.array(mds1[ptx][XT]['longitude'],\n",
    "                fill_value=attrs1[ptx][XT]['longitude']['_FillValue'])\n",
    "            latitude['XT'].mask = (latitude['XT'] == longitude['XT'].fill_value)\n",
    "            delta_time['XT'] = np.ma.array(mds1[ptx][XT]['delta_time'],\n",
    "                fill_value=attrs1[ptx][XT]['delta_time']['_FillValue'])\n",
    "            delta_time['XT'].mask = (delta_time['XT'] == delta_time['XT'].fill_value)\n",
    "            # corrected height\n",
    "            h_corr['XT'] = np.ma.array(mds1[ptx][XT]['h_corr'],\n",
    "                fill_value=attrs1[ptx][XT]['h_corr']['_FillValue'])\n",
    "            h_corr['XT'].mask = (h_corr['XT'] == h_corr['XT'].fill_value)\n",
    "            # quality summary\n",
    "            quality_summary['XT'] = (mds1[ptx][XT]['atl06_quality_summary'] == 0)\n",
    "            # ocean corrections\n",
    "            for m in TIDE_MODELS:\n",
    "                tide_ocean[m]['XT'] = np.ma.zeros((n_cross),\n",
    "                    fill_value=attrs1[ptx][XT]['tide_ocean']['_FillValue'])\n",
    "                tide_ocean[m]['XT'].mask = np.zeros((n_cross),dtype=bool)\n",
    "            ib['XT'] = np.ma.array(mds1[ptx][XT]['dac'],\n",
    "                fill_value=attrs1[ptx][XT]['dac']['_FillValue'])\n",
    "            ib['XT'].mask = (ib['XT'] == ib['XT'].fill_value)\n",
    "\n",
    "        # ice shelf mask\n",
    "        a2 = (PRD,'ICE_SHELF','MASK',TRK,GRAN,SCYC,ECYC,RL,VERS,AUX)\n",
    "        f2 = os.path.join(base_dir,file_format.format(*a2))\n",
    "        # create data mask for ice shelves\n",
    "        mds1[ptx]['subsetting'] = {}\n",
    "        mds1[ptx]['subsetting'].setdefault('ice_shelf',\n",
    "            np.zeros((n_points),dtype=bool))\n",
    "        # check that mask file exists\n",
    "        try:\n",
    "            mds2,attr2 = read_HDF5_ATL11_pair(f2,ptx,\n",
    "                VERBOSE=False,SUBSETTING=True)\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            mds1[ptx]['subsetting']['ice_shelf'] = \\\n",
    "                mds2[ptx]['subsetting']['ice_shelf']\n",
    "\n",
    "        # read height corrections from each tide model\n",
    "        for m in TIDE_MODELS:\n",
    "            # tide model\n",
    "            a3 = (PRD,m,'TIDES',TRK,GRAN,SCYC,ECYC,RL,VERS,AUX)\n",
    "            f3 = os.path.join(base_dir,file_format.format(*a3))\n",
    "            # check that tide file exists\n",
    "            try:\n",
    "                mds3,attr3 = read_HDF5_ATL11_pair(f3,ptx,\n",
    "                    VERBOSE=False,CROSSOVERS=CROSSOVERS)\n",
    "            except:\n",
    "                # mask all values\n",
    "                for group in groups:\n",
    "                    tide_ocean[m][group].mask[:] = True\n",
    "                continue\n",
    "            else:\n",
    "                tide_ocean[m]['AT'].data[:] = \\\n",
    "                        mds3[ptx]['cycle_stats']['tide_ocean']\n",
    "                if CROSSOVERS:\n",
    "                    tide_ocean[m]['XT'].data[:] = \\\n",
    "                        mds3[ptx][XT]['tide_ocean']\n",
    "            # set masks and fill values\n",
    "            for group,val in tide_ocean[m].items():\n",
    "                val.mask[:] = (val.data == val.fill_value)\n",
    "                val.data[val.mask] = val.fill_value\n",
    "\n",
    "        #-- check method of differencing\n",
    "        if (METHOD == 'AT'):\n",
    "            # if running along-track differences\n",
    "            difference_cycles = np.arange(n_cycles-1)\n",
    "            n_diff = np.copy(n_points)\n",
    "            # convert lat/lon to polar stereographic\n",
    "            X,Y = transformer.transform(longitude['AT'],latitude['AT'])\n",
    "            # run for all indices\n",
    "            ref_indices = Ellipsis\n",
    "        elif (METHOD == 'XT'):\n",
    "            # if running crossovers\n",
    "            difference_cycles = np.arange(n_cycles)\n",
    "            n_diff = np.copy(n_cross)\n",
    "            # convert lat/lon to polar stereographic\n",
    "            X,Y = transformer.transform(longitude['XT'],latitude['XT'])\n",
    "            # find mapping between crossover and along-track reference points\n",
    "            ref_indices = common_reference_points(ref_pt['XT'], ref_pt['AT'])\n",
    "        else:\n",
    "            difference_cycles = []\n",
    "        # for each cycle\n",
    "        for cycle in difference_cycles:\n",
    "            # fill value for invalid values\n",
    "            fv = attrs1[ptx]['h_corr']['_FillValue']\n",
    "            # copy annual land ice height variables\n",
    "            h1 = np.ma.array(mds1[ptx]['h_corr'][ref_indices,cycle],\n",
    "                fill_value=fv)\n",
    "            if CROSSOVERS:\n",
    "                h2 = np.ma.array(mds1[ptx][XT]['h_corr'][:],\n",
    "                    fill_value=fv)\n",
    "            else:\n",
    "                h2 = np.ma.array(mds1[ptx]['h_corr'][:,cycle+1],\n",
    "                    fill_value=fv)\n",
    "\n",
    "            # create masks for height variables\n",
    "            h1.mask = (h1.data == h1.fill_value)\n",
    "            h2.mask = (h2.data == h2.fill_value)\n",
    "\n",
    "            # # reference heights to geoid\n",
    "            # h1 -= mds1[ptx]['ref_surf']['geoid_h']\n",
    "            # h2 -= mds1[ptx]['ref_surf']['geoid_h'][ref_indices]\n",
    "            # correct heights for ocean variability\n",
    "            h1 -= ib['AT'][ref_indices,cycle]\n",
    "            if CROSSOVERS:\n",
    "                h2 -= ib['XT'][:]\n",
    "            else:\n",
    "                h2 -= ib['AT'][:,cycle+1]\n",
    "\n",
    "            # calculate corrected height differences\n",
    "            h_diff = np.ma.zeros((n_diff),fill_value=fv)\n",
    "            # set masks for invalid points\n",
    "            h_diff.mask = np.zeros((n_diff),dtype=bool)\n",
    "            # check if data is valid and within bounds\n",
    "            h_diff.mask |= (h1.mask | h2.mask)\n",
    "            h_diff.mask |= (np.abs(h1 - h2) > np.abs(vmin)) | \\\n",
    "                (np.abs(h1 - h2) > np.abs(vmax))\n",
    "            # check if tide model is valid\n",
    "            for m in TIDE_MODELS:\n",
    "                h_diff.mask |= tide_ocean[m]['AT'].mask[ref_indices,cycle]\n",
    "                if CROSSOVERS:\n",
    "                    h_diff.mask |= tide_ocean[m]['XT'].mask[:]\n",
    "                else:\n",
    "                    h_diff.mask |= tide_ocean[m]['AT'].mask[:,cycle+1]\n",
    "            # check if IB correction is valid\n",
    "            h_diff.mask |= ib['AT'].mask[:,cycle]\n",
    "            if CROSSOVERS:\n",
    "                h_diff.mask |= ib['XT'].mask[:]\n",
    "            else:\n",
    "                h_diff.mask |= ib['AT'].mask[:,cycle+1]\n",
    "            # check if a low quality surface fit\n",
    "            h_diff.mask |= np.logical_not(quality_summary['AT'][ref_indices,cycle])\n",
    "            if CROSSOVERS:\n",
    "                h_diff.mask |= np.logical_not(quality_summary['XT'][:])\n",
    "            else:\n",
    "                h_diff.mask |= np.logical_not(quality_summary['AT'][:,cycle+1])\n",
    "            # check if not ice shelf\n",
    "            subset_mask = mds1[ptx]['subsetting']['ice_shelf'][ref_indices]\n",
    "            h_diff.mask |= np.logical_not(subset_mask)\n",
    "            # check if below height threshold\n",
    "            h_diff.mask |= (h1 <= THRESHOLD) | (h2 <= THRESHOLD)\n",
    "            # check if points are within bounds of plot\n",
    "            h_diff.mask |= (X < region_xlimit[REGION][0]) | \\\n",
    "                (X > region_xlimit[REGION][1]) | \\\n",
    "                (Y < region_ylimit[REGION][0]) | \\\n",
    "                (Y > region_ylimit[REGION][1])\n",
    "            # check if crossover measurements are within time range\n",
    "            if CROSSOVERS:\n",
    "                # check versus threshold in days\n",
    "                dt = (delta_time['XT'] - delta_time['AT'][ref_indices,cycle])\n",
    "                h_diff.mask |= (np.abs(dt/86400.0) > DAYS)\n",
    "                h_diff.mask |= delta_time['AT'].mask[ref_indices,cycle]\n",
    "                h_diff.mask |= delta_time['XT'].mask[:]\n",
    "\n",
    "            # calculate elevation histogram for beam\n",
    "            if np.any(~h_diff.mask):\n",
    "                # calculate height difference\n",
    "                h_diff.data[:] = h2.data[:] - h1.data[:]\n",
    "                # histogram using numpy\n",
    "                hh,hb = np.histogram(h_diff.compressed(),bins=b1)\n",
    "                # add to total uncorrected histogram\n",
    "                hist['Uncorrected'] += hh.astype(np.float)\n",
    "                #-- calculate tide-corrected height differences\n",
    "                for m in TIDE_MODELS:\n",
    "                    to1 = np.copy(tide_ocean[m]['AT'][ref_indices,cycle])\n",
    "                    if CROSSOVERS:\n",
    "                        to2 = np.copy(tide_ocean[m]['XT'][:])\n",
    "                    else:\n",
    "                        to2 = np.copy(tide_ocean[m]['AT'][:,cycle+1])\n",
    "                    # tide-corrected height difference\n",
    "                    h_diff.data[:] = (h2 - to2) - (h1 - to1)\n",
    "                    # histogram using numpy\n",
    "                    hh,hb = np.histogram(h_diff.compressed(),bins=b1)\n",
    "                    # add to total histogram\n",
    "                    hist[m] += hh.astype(np.float)\n",
    "                # add to valid segments\n",
    "                total_valid += np.count_nonzero(~h_diff.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure axes for output histogram plots\n",
    "fig,ax = plt.subplots(ncols=5, sharey=True, figsize=(11,4))\n",
    "plot_labels = ['a)','b)','c)','d)','e)']\n",
    "\n",
    "# output file of histogram statistics\n",
    "args = ('ATL11',METHOD,REGION,RELEASE)\n",
    "FILE = '{0}_{1}_{2}_TIDAL_HISTOGRAM_{3}.txt'.format(*args)\n",
    "fid = open(os.path.join(base_dir,FILE),'w')\n",
    "print('Histogram Statistics ({0})'.format(METHOD), file=fid)\n",
    "print('Minimum: {0:0.2f}'.format(vmin), file=fid)\n",
    "print('Maximum: {0:0.2f}'.format(vmax), file=fid)\n",
    "print('Width: {0:0.2f}'.format(w), file=fid)\n",
    "print('Bins: {0:d}'.format(nbins), file=fid)\n",
    "# print total number of points for differencing method\n",
    "print('All Cycles: {0:d}\\n'.format(total_valid), file=fid)\n",
    "\n",
    "# create histogram plots\n",
    "for i,key in enumerate(['Uncorrected',*TIDE_MODELS]):\n",
    "    # plot histograms\n",
    "    ax[i].plot(b2,hist[key],color='darkorchid',lw=1.5)\n",
    "    ax[i].fill_between(b2,hist[key],color='darkorchid',alpha=0.5)\n",
    "    # set title\n",
    "    ax[i].set_title(key)\n",
    "    # Add figure label\n",
    "    at = matplotlib.offsetbox.AnchoredText(plot_labels[i], loc=2, pad=0,\n",
    "        frameon=False, prop=dict(size=14,weight='bold',color='k'))\n",
    "    ax[i].axes.add_artist(at)\n",
    "    # add x labels\n",
    "    ax[i].set_xlabel('Elevation Difference [m]',labelpad=3)\n",
    "    # calculate histogram statistics\n",
    "    N = np.sum(hist[key])\n",
    "    # histogram mean and standard deviation\n",
    "    hmean = np.average(b2, weights=hist[key])\n",
    "    hvariance = np.average((b2-hmean)**2, weights=hist[key])\n",
    "    hstdev = np.sqrt(hvariance)\n",
    "    # histogram skewness and excess kurtosis\n",
    "    hskewness = np.average((b2-hmean)**3, weights=hist[key])/(hstdev**3)\n",
    "    hkurtosis = np.average((b2-hmean)**4, weights=hist[key])/(hstdev**4)\n",
    "    hkurtosis_excess = hkurtosis - 3.0\n",
    "    # omnibus chi-squared test of normality\n",
    "    mu1 = np.sqrt(6.0*N*(N-1.0)/(N-2.0)/(N+1.0)/(N+3.0))\n",
    "    mu2 = 2.0*mu1*np.sqrt((N*N-1.0)/(N-3.0)/(N+5.0))\n",
    "    chi2 = (hskewness/mu1)**2 + (hkurtosis_excess/mu2)**2\n",
    "    pvalue = 1.0 - scipy.stats.chi2.cdf(chi2,2)\n",
    "    # cumulative probability distribution function of histogram\n",
    "    cpdf = np.cumsum(hist[key]/np.sum(hist[key]))\n",
    "    # calculate percentiles for IQR and RDE\n",
    "    # IQR: first and third quartiles (25th and 75th percentiles)\n",
    "    # RDE: 16th and 84th percentiles\n",
    "    # median: 50th percentile\n",
    "    Q1,Q3,P16,P84,hmedian = np.interp([0.25,0.75,0.16,0.84,0.5],cpdf,b2)\n",
    "    # calculate interquartile range (IQR)\n",
    "    hIQR = 0.75*(Q3 - Q1)\n",
    "    # calculate robust dispersion estimator (RDE)\n",
    "    hRDE = 0.50*(P84 - P16)\n",
    "    # print model to file\n",
    "    print('{0}:'.format(key), file=fid)\n",
    "    # print statistics to file\n",
    "    print('\\t{0}: {1:f}'.format('Mean',hmean), file=fid)\n",
    "    print('\\t{0}: {1:f}'.format('Median',hmedian), file=fid)\n",
    "    print('\\t{0}: {1:f}'.format('StDev',hstdev), file=fid)\n",
    "    print('\\t{0}: {1:f}'.format('Skewness',hskewness), file=fid)\n",
    "    print('\\t{0}: {1:f}'.format('Kurtosis',hkurtosis_excess), file=fid)\n",
    "    # print('\\t{0}: {1:f}'.format('Normal',pvalue), file=fid)\n",
    "    # print median statistics to file\n",
    "    print('\\t{0}: {1:f}'.format('IQR',hIQR), file=fid)\n",
    "    print('\\t{0}: {1:f}'.format('RDE',hRDE), file=fid)\n",
    "    # output file of histogram for model\n",
    "    args = ('ATL11',METHOD,REGION,key,RELEASE)\n",
    "    HIST = '{0}_{1}_{2}_{3}_TIDAL_HISTOGRAM_{4}.txt'.format(*args)\n",
    "    fid1 = open(os.path.join(base_dir,HIST),'w')\n",
    "    # for each histogram bin\n",
    "    for bn,hst in zip(b2,hist[key]):\n",
    "        print('{0:0.02f} {1:0.0f}'.format(bn,hst),file=fid1)\n",
    "    # close model histogram file\n",
    "    fid1.close()\n",
    "# close statistics file\n",
    "fid.close()\n",
    "\n",
    "# add y labels\n",
    "ax[0].set_ylabel('Count', labelpad=10)\n",
    "# set ylimits\n",
    "ymin,ymax = ax[0].get_ylim()\n",
    "ax[0].set_ylim(0,ymax)\n",
    "# adjust plot to figure dimensions\n",
    "fig.subplots_adjust(left=0.07,right=0.98,top=0.93,bottom=0.11,\n",
    "    wspace=0.12,hspace=0.20)\n",
    "# output file format for each region type\n",
    "args = ('ATL11',METHOD,REGION,RELEASE)\n",
    "PLOT = '{0}_{1}_{2}_TIDAL_HISTOGRAM_{3}.pdf'.format(*args)\n",
    "# save plot as png to the plot directory\n",
    "print('\\t--> {0}'.format(os.path.join(base_dir,PLOT)))\n",
    "plt.savefig(os.path.join(base_dir,PLOT), format='pdf', dpi=720,\n",
    "    metadata={'Title':os.path.basename(sys.argv[0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure axes for merged histogram plots\n",
    "fig,ax = plt.subplots(num=2,figsize=(6,6))\n",
    "# inset axes\n",
    "axins = ax.inset_axes([0.65, 0.65, 0.33, 0.33])\n",
    "# plot colors for histograms\n",
    "COLORS = ['0.2','darkorchid','mediumseagreen','darkorange','dodgerblue']\n",
    "# create merged histogram plot\n",
    "for i,key in enumerate(['Uncorrected',*TIDE_MODELS]):\n",
    "    # plot histograms\n",
    "    ax.plot(hist[:,0],hist[:,1],color=COLORS[i],lw=1.5, label=key)\n",
    "    axins.plot(hist[:,0],hist[:,1],color=COLORS[i],lw=1.5)\n",
    "\n",
    "# add legend\n",
    "lgd = ax.legend(loc=2,frameon=False)\n",
    "lgd.get_frame().set_alpha(1.0)\n",
    "for line in lgd.get_lines():\n",
    "    line.set_linewidth(6)\n",
    "\n",
    "# create sub region of the original plot\n",
    "x1, x2, y1, y2 = (-0.10, 0.10, 260000, 275000)\n",
    "axins.set_xlim(x1, x2)\n",
    "axins.set_ylim(y1, y2)\n",
    "axins.set_xticklabels('')\n",
    "axins.set_yticklabels('')\n",
    "ax.indicate_inset_zoom(axins)\n",
    "\n",
    "# add x and y labels\n",
    "ax.set_xlabel('Elevation Difference [m]',labelpad=3)\n",
    "ax.set_ylabel('Count', labelpad=10)\n",
    "# set ylimits\n",
    "ymin,ymax = ax.get_ylim()\n",
    "ax.set_ylim(0,ymax)\n",
    "# adjust plot to figure dimensions\n",
    "fig.subplots_adjust(left=0.14,right=0.98,top=0.98,bottom=0.08)\n",
    "\n",
    "# output file format\n",
    "args = ('ATL11',METHOD,REGION,RELEASE)\n",
    "PLOT = '{0}_{1}_{2}_TIDAL_HISTOGRAM_{3}_single.pdf'.format(*args)\n",
    "# save plot as png to the plot directory\n",
    "print('\\t--> {0}'.format(os.path.join(base_dir,PLOT)))\n",
    "plt.savefig(os.path.join(base_dir,PLOT), format='pdf', dpi=720,\n",
    "    metadata={'Title':os.path.basename(sys.argv[0])})"
   ]
  }
 ]
}